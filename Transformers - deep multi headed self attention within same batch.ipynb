{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "df984252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  1.10.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "print(\"PyTorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3f878037",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "with open('../karpathy-ai/m-gpt/input.txt', encoding = 'utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# create mapping from charcter to integer\n",
    "stoi = {ch: i for  i,ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "# train and test splits\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "49a7826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_embed, context_length):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.head_size = n_embed // n_heads\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((context_length, context_length))).view(1,1, context_length, context_length))\n",
    "        self.c_atten = nn.Linear(n_embed, 3 * n_embed, bias = False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"X.shape at mhsa: \", x.shape)\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.c_atten(x).split(self.n_embed, dim = 2)\n",
    "        q = q.view(B, T, n_heads, self.head_size).transpose(1, 2) # B, hd, T, hs\n",
    "        k = k.view(B, T, n_heads, self.head_size).transpose(1, 2) # B, hd, T, hs\n",
    "        v = v.view(B, T, n_heads, self.head_size).transpose(1, 2) # B, hd, T, hs\n",
    "        wei = k @ q.transpose(-2, -1) # (B, hd, T, hs) @ (B, hd, hs, T) = (B, hd, T, T)\n",
    "        wei = F.softmax(wei.masked_fill(self.tril[:, :, :T, :T] == 0, float('-inf')) / math.sqrt(k.size(-1)), dim = -1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v # (B, hd, T, T) @ B, hd, T, hs) = (B, hd, T, hs)\n",
    "        return out.transpose(1,2).contiguous().view(B, T, C)\n",
    "\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_heads, n_embed, context_length):\n",
    "        super().__init__()\n",
    "        self.mhsa = MultiHeadedSelfAttention(n_heads, n_embed, context_length)\n",
    "        self.llnorm_1 = nn.LayerNorm(n_embed)\n",
    "        self.llnorm_2 = nn.LayerNorm(n_embed)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.mhsa(self.llnorm_1(x))\n",
    "        ffwd = self.ffwd(self.llnorm_2(x))\n",
    "        x = x + ffwd\n",
    "        return x\n",
    "    \n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, n_heads, vocab_size , n_embed, context_length, n_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding = nn.Embedding(context_length, n_embed)\n",
    "        self.transformer = nn.ModuleList([Block(n_heads, n_embed, context_length) for _ in range(n_layers)])\n",
    "        self.llm_head = nn.Linear(n_embed, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        val_embd = self.embedding(x)\n",
    "        pos_embd = self.position_embedding(torch.arange(0, T))\n",
    "        x = val_embd + pos_embd\n",
    "        for block in self.transformer:\n",
    "            x = block(x)\n",
    "        output = self.llm_head(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "4bf09ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 4\n",
    "n_embed = 32\n",
    "context_length = 50\n",
    "vocab_size = 65\n",
    "batch_size = 64\n",
    "n_layers = 3\n",
    "token_dim = 1\n",
    "eval_interval = 100\n",
    "eval_iteration = 100\n",
    "iterations = 5000\n",
    "lr = 1e-3\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "x = torch.randint(vocab_size, (batch_size, context_length, ))\n",
    "\n",
    "gpt = GPT(n_heads = n_heads, \n",
    "                          vocab_size = vocab_size, \n",
    "                          n_embed = n_embed, \n",
    "                          context_length = context_length,\n",
    "                         n_layers = n_layers)\n",
    "\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c35b18b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model paramters:  0.028929 M\n"
     ]
    }
   ],
   "source": [
    "print(\"Model paramters: \", sum(p.nelement() for p in gpt.parameters()) / 1e6, \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bfde4fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 1 iterations - train loss: 4.5124, val loss: 4.5246\n",
      "Loss at 101 iterations - train loss: 2.8582, val loss: 2.8878\n",
      "Loss at 201 iterations - train loss: 2.6429, val loss: 2.6631\n",
      "Loss at 301 iterations - train loss: 2.5628, val loss: 2.5680\n",
      "Loss at 401 iterations - train loss: 2.5124, val loss: 2.5184\n",
      "Loss at 501 iterations - train loss: 2.4762, val loss: 2.4818\n",
      "Loss at 601 iterations - train loss: 2.4387, val loss: 2.4478\n",
      "Loss at 701 iterations - train loss: 2.4089, val loss: 2.4188\n",
      "Loss at 801 iterations - train loss: 2.3821, val loss: 2.3956\n",
      "Loss at 901 iterations - train loss: 2.3597, val loss: 2.3667\n",
      "Loss at 1001 iterations - train loss: 2.3352, val loss: 2.3475\n",
      "Loss at 1101 iterations - train loss: 2.3078, val loss: 2.3223\n",
      "Loss at 1201 iterations - train loss: 2.2836, val loss: 2.3024\n",
      "Loss at 1301 iterations - train loss: 2.2598, val loss: 2.2834\n",
      "Loss at 1401 iterations - train loss: 2.2421, val loss: 2.2665\n",
      "Loss at 1501 iterations - train loss: 2.2242, val loss: 2.2453\n",
      "Loss at 1601 iterations - train loss: 2.2034, val loss: 2.2323\n",
      "Loss at 1701 iterations - train loss: 2.1909, val loss: 2.2156\n",
      "Loss at 1801 iterations - train loss: 2.1692, val loss: 2.1970\n",
      "Loss at 1901 iterations - train loss: 2.1547, val loss: 2.1861\n",
      "Loss at 2001 iterations - train loss: 2.1388, val loss: 2.1753\n",
      "Loss at 2101 iterations - train loss: 2.1217, val loss: 2.1631\n",
      "Loss at 2201 iterations - train loss: 2.1095, val loss: 2.1476\n",
      "Loss at 2301 iterations - train loss: 2.0984, val loss: 2.1424\n",
      "Loss at 2401 iterations - train loss: 2.0824, val loss: 2.1247\n",
      "Loss at 2501 iterations - train loss: 2.0664, val loss: 2.1177\n",
      "Loss at 2601 iterations - train loss: 2.0568, val loss: 2.1082\n",
      "Loss at 2701 iterations - train loss: 2.0471, val loss: 2.0906\n",
      "Loss at 2801 iterations - train loss: 2.0427, val loss: 2.0887\n",
      "Loss at 2901 iterations - train loss: 2.0253, val loss: 2.0834\n",
      "Loss at 3001 iterations - train loss: 2.0168, val loss: 2.0675\n",
      "Loss at 3101 iterations - train loss: 2.0073, val loss: 2.0583\n",
      "Loss at 3201 iterations - train loss: 2.0081, val loss: 2.0662\n",
      "Loss at 3301 iterations - train loss: 1.9863, val loss: 2.0462\n",
      "Loss at 3401 iterations - train loss: 1.9836, val loss: 2.0443\n",
      "Loss at 3501 iterations - train loss: 1.9766, val loss: 2.0336\n",
      "Loss at 3601 iterations - train loss: 1.9710, val loss: 2.0254\n",
      "Loss at 3701 iterations - train loss: 1.9612, val loss: 2.0275\n",
      "Loss at 3801 iterations - train loss: 1.9541, val loss: 2.0240\n",
      "Loss at 3901 iterations - train loss: 1.9459, val loss: 2.0093\n",
      "Loss at 4001 iterations - train loss: 1.9432, val loss: 2.0135\n",
      "Loss at 4101 iterations - train loss: 1.9304, val loss: 2.0027\n",
      "Loss at 4201 iterations - train loss: 1.9265, val loss: 1.9998\n",
      "Loss at 4301 iterations - train loss: 1.9245, val loss: 2.0010\n",
      "Loss at 4401 iterations - train loss: 1.9211, val loss: 1.9969\n",
      "Loss at 4501 iterations - train loss: 1.9127, val loss: 1.9909\n",
      "Loss at 4601 iterations - train loss: 1.9083, val loss: 1.9970\n",
      "Loss at 4701 iterations - train loss: 1.9068, val loss: 1.9839\n",
      "Loss at 4801 iterations - train loss: 1.8988, val loss: 1.9743\n",
      "Loss at 4901 iterations - train loss: 1.8895, val loss: 1.9818\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    gpt.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iteration)\n",
    "        for k in range(eval_iteration):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits = gpt(xb)\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, C), yb.view(B*T))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    gpt.train()\n",
    "    return out\n",
    "\n",
    "def forward_pass_with_grad(xb, yb):\n",
    "    gpt.train()\n",
    "    logits = gpt(xb)\n",
    "    B, T, C = logits.shape\n",
    "    loss = F.cross_entropy(logits.view(B*T, C), yb.view(B*T))\n",
    "    return loss, logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_pass_without_grad(xb):\n",
    "    gpt.eval()\n",
    "    logits = gpt(xb)\n",
    "    return logits\n",
    "\n",
    "def get_batch(split = 'train'):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i: i+ context_length] for i in ix])\n",
    "    y = torch.stack([data[i+1 : i + context_length + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    if i % eval_interval == 0:\n",
    "        loss = evaluate()\n",
    "        print(f\"Loss at {i+1} iterations - train loss: {loss['train']:.4f}, val loss: {loss['val']:.4f}\")\n",
    "    else:\n",
    "        xb, yb = get_batch(split = 'train')\n",
    "        loss, logits = forward_pass_with_grad(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d57fc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(tokens, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        tokens_cond = tokens[:, -context_length:]\n",
    "        logits = forward_pass_without_grad(tokens_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        next_token = torch.multinomial(probs, num_samples = 1)\n",
    "        tokens = torch.cat((tokens, next_token), dim = 1)\n",
    "    return decode(tokens[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1a183f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "st yeciogire head gue of muttrie.\n",
      "\n",
      "BUKENENCESCERY:\n",
      "I but, I both ping of theos: you have,\n",
      "Whout wure\n",
      "Seat may brof.\n",
      "\n",
      "BARDUCHENGA:\n",
      "Forrrous of thise proting'd past of colvet will rechert\n",
      "Is it hang hattmy that shas pose;\n",
      "That if me but betaresh'd my shen.\n",
      "\n",
      "KINCENN:\n",
      "Whet it I mid lerives ugot, is bek'd,\n",
      "Abunw you like pagaare frooms; faar my it ga;\n",
      "Aft Ifal is molid, withr my lome\n",
      "feack land that Pedien, her moes; pheant,\n",
      "How the compis, this, of thy sust non.\n",
      "\n",
      "Pon QUEEjon,'se bit co yound Cheall, nown\n",
      "Fir bothere be loss, tray knaXIIZENT:\n",
      "And the Jepuly the wortanst, it.\n",
      "\n",
      "SICHARD BEOLLO:\n",
      "I he hovaratin!\n",
      "In:\n",
      "Lat what theise parpt\n",
      "And iblie, with or my lisce: dentie;\n",
      "Hid they shastaling! Vesslaing the have awitith ibeabod.\n",
      "\n",
      "FRCHICH I RICIIO\n",
      "\n",
      "NIIO:\n",
      "The gault:\n",
      "Gam whan fing will you this mave my,\n",
      "A mainy nor:\n",
      "Bet\n",
      "Gef and have and at that be altle sen. mike;\n",
      "And is, whe &hf it? Muthwen lim, I Enot!\n",
      "\n",
      "Nord, shall theine peprowill love, witht lle\n",
      "And ting I steen!\n",
      "But noth? Hat staul gitteing, ten, as there to mudes a tho comporrded in aw a I your sirfretiuns I fand\n",
      "rat but a shallst we makelookat; I', ic as Lation I to courter proince of\n",
      "ever a hown hise im\n",
      "on, Son on. I logaid now,\n",
      "All a yince and shall wall mat, theee and lerfade thurew. the rlaves,\n",
      "Othan all held He, awheen thim not retseing te much inse of lovere,\n",
      "Ord, frow had itgliave? Orn your that\n",
      "Wicle world be cofiait! my I therow,\n",
      "Fagixt, brouraty donown.\n",
      "\n",
      "Sakewpilining of who, thou.\n",
      "Your my to Were this ou, noot prume moves, if ole,\n",
      "When to my priced, and be dery,\n",
      "The I rid, by Rorfort:\n",
      "Qucht that cruke:\n",
      "That lien out me\n",
      "and ruinge jut, thider hhis sell dien.\n",
      "\n",
      "SMARCK:\n",
      "And'ty, thou thou the hom frayeiced, the You they, in a le\n",
      "on on hock whonet opareare so,\n",
      "For dear the me weeh, lord-nows ksven a will hingback!\n",
      "Whis for you, I will!\n",
      "I no this moe; hefe users, a prame ande,\n",
      "That fall mind a'lst wils ill coufnd: it more's an to maffe mace'd.\n",
      "Thoses dot hattelund the more cwibroust.\n",
      "\n",
      "FROMINTE:\n",
      "My feare actur of the plare coself\n",
      "I his; if imppy hels pity:-I mid the salm that doint.\n",
      " patted, hath Poorenst? my llove.\n",
      "Apth you yaurd no growns theett.\n",
      "\n",
      "KING ENSBusen'd thou on--how 't, well did\n",
      "We yom: bave ack,  as stak, factenst our hilds: your a sy fropeem.\n",
      "\n",
      "SICLTHARD VII:\n",
      "Sir, may cuch rejoty uren am cover?\n",
      "\n",
      "That Mardown! I mast;? I will knoth weat ings never the is trog eing tyou ter:\n",
      "I alend your the is be.\n",
      "But do atheir-l to sfor the -here pholam.\n",
      "\n",
      "PLIICENGE:\n",
      "Why to wathse stroungst perber for suth makoututy, go in dethe.\n",
      "Say coour be id fire, be I'll trut If brovelt a? marqued,\n",
      "Mavey searet, weare shimflamning? I his and with Lord,\n",
      "Ther like thal sameas bleake. knone what\n",
      "And comen the that on of it ustrel,\n",
      "But well a driffed hem stake as the cup,\n",
      "Thurk's that Whing have may hurmen.\n",
      "\n",
      "KING RICHAS INGIA:\n",
      "O are a to titnife more\n",
      "and ien what califfe; me lattedly butte. Britgae.\n",
      "\n",
      "KING LI:\n",
      "Birst thi's bet with, his lesath when upow\n",
      "And ose thou meat brut haves falt' thy tem is tho's picple\n",
      "Sotins me blast morench peacione dee'd uman:\n",
      "Whesere ill to cand sworrw\n",
      "loroo; the If the dist?\n",
      "\n",
      "BOLISTHCINT:\n",
      "Whill; I hemand, lord, Gou, a wit.\n",
      "\n",
      "KING E VIINGBRY:\n",
      "Sing piset Pellaray, of I his to deavin's live to cinedd rewn my the we tide fare with.\n",
      "But lose hey past. Teparr sur You wank\n",
      "How the you lood lamm my tho spin Conflay her well me buteafeiths I oner, gether,\n",
      "For come hicking shoreld merrish beot you lre faice,\n",
      "Than me on. O unth gure see us plance me! Forg:\n",
      "You cift, bet, I than is love, their,\n",
      "Did! feaicte.\n",
      "\n",
      "DLIUMNIIE:\n",
      "Thay, he yould a je fool, this, bid an sibang thas sown'd, mifein, saride the'erar.\n",
      "\n",
      "JUPUS:\n",
      "Fill mige:\n",
      "Heour tho-gews air with ear. Maysoret?\n",
      "\n",
      "CARDY:\n",
      "I proth hand tham, love meatake per'd your, lese a thy gupon,\n",
      "Of mistance your, stall this Kearr\n",
      "So more semikant as that us fware\n",
      "Tith withot polaices!-\n",
      "ESCIONTES:\n",
      "Sark', 'ch say onot a but aficte froussly!\n",
      "\n",
      "MENO:\n",
      "As will surt himvul home prigivore\n",
      "Af ham pleen hoter, it wilth looke aging,\n",
      "And ghandy sir: has doth your fream's poorean bech.\n",
      "\n",
      "COROKIO:\n",
      "SLorains, may matend so hous is of than our sleall us wife?\n",
      "\n",
      "FRGBRICHASTAS:\n",
      "You of, Welll zounk to the like it an to,\n",
      "I will hove ped: what I, that his mack I his a'tus wees.\n",
      "\n",
      "COLONCIO:\n",
      "O swhick, is\n",
      "And foreesior, not fam would you baye;\n",
      "for beacter'd onceion.\n",
      "\n",
      "Quidlisest hall you me of word, theirs.\n",
      " up is Enowns slauce pwile know thill bawss befoneshfull you the ume hande,\n",
      "Yef hee to niespurt this thy stuld of that wone\n",
      "Lerink thou live arot! not what 'bloode the we borrour for knowlang whom wastin of af then Erequirs\n",
      "And then,\n",
      "And lowguive eapp? your, But How, I heill,\n",
      "Thus we as will; alere, is uprrame ball vy of of thus hong't all may cark,\n",
      "Thinks hear pild hiw thill up!\n",
      "\n",
      "GOF'ZAS:\n",
      "Host I in fall--oe not: I hecrep heam thy pray Cliked.\n",
      "Romen: my by Hats: is cuist to wort,\n",
      "Foroong my wash urroim wich you,\n",
      "Nur what, beir wars is as, at'ty painites misk.\n",
      "\n",
      "WARWICK:\n",
      "I it were, his cition:\n",
      "my, so brow lo; hafoake as and love mastelf\n",
      "If 'To sodeime hord. Or\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype = torch.long)\n",
    "print(generate(context, 5000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
